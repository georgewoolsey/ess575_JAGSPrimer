---
title: "ESS 575: Bayes Theorem Lab"
author: "Team England" 
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    toc: false
    toc_depth: 3
linkcolor: blue
header-includes:
  - \usepackage{caption}
  - \captionsetup[figure]{labelformat=empty}
editor_options: 
  chunk_output_type: console
knit: (function(inputFile, encoding){ 
    out_dir <- '../';
    rmarkdown::render(inputFile, encoding = encoding, output_file=file.path(dirname(inputFile), out_dir, 'JAGSPrimer_England.pdf')) 
  })
---

Team England:

  - Caroline Blommel
  - Carolyn Coyle
  - Bryn Crosby
  - George Woolsey
  
cblommel@mail.colostate.edu, carolynm@mail.colostate.edu, brcrosby@rams.colostate.edu, george.woolsey@colostate.edu

# Setup

Download the R package [SESYNCBayes ver. 0.6](https://cchecastaldo.github.io/BayesianShortCourse/content/package/SESYNCBayes_0.6.0.tar.gz) to your computer.

Run:

`install.packages("<pathtoSESYNCBayes>/Packages/SESYNCBayes_0.6.0.tar.gz", repos = NULL, type = "source")`

```{r setup, include=F}
## load packages
library(tidyverse)
library(lubridate)
library(viridis)
library(scales)
library(latex2exp)
# visualization
library(ggpubr)
library(cowplot)
library(kableExtra)
# jags
library(rjags)
library(SESYNCBayes)
library(MCMCvis)
library(HDInterval)
library(coda)

# knit options
knitr::opts_chunk$set(
  echo = TRUE
  , warning = FALSE
  , message = FALSE
  , fig.height = 5
  , fig.width = 7
)

```

# Aim

The purpose of [this Primer](https://nthobbs50.github.io/ESS575/content/lectures/JAGSPrimerMCMCVis.pdf) is to teach the programming skills needed to approximate the marginal posterior distributions of parameters, latent variables, and derived quantities of interest using software implementing Markov chain Monte Carlo methods. Along the way, we will reinforce some of the ideas and principles that we have learned in lecture. The Primer is organized primarily as a tutorial and contains only a modicum of reference material. You will need to study the JAGS manual at some point to become a fully competent programmer.

# Example

Consider a model predicting the per-capita rate of population growth (or the mass specific
rate of nitrogen accumulation),

\begin{equation}
\frac{1}{N}\frac{dN}{dt}=r-\frac{r}{K}N,
\end{equation}

which, of course, is a linear model with intercept $r$ and slope $\frac{r}{K}$. Note that these quantities enjoy a sturdy biological interpretation in population ecology; $r$ is the intrinsic rate of increase, $\frac{r}{K}$ is the strength of the feedback from population size to population growth rate, and $K$ is the carrying capacity, that is, the population size (o.k., o.k., the gm $N$ per gm soil for the ecosystem scientists) at which $\frac{}{}$$\frac{dN}{dt}=0$. Presume we have some data consisting of observations of per capita rate of growth paired with observations of $N$. The vector $\mathbf{y}$ contains values for the rate and the vector $\mathbf{x}$ contains aligned data on $N$, i.e., $y_{i}=\frac{1}{N_{i}}\frac{dN_{i}}{dt},\, x_{i}=N_{i}$. To keep things simple, we start out by assuming that the $x_{i}$ are measured without error. A simple Bayesian model specifies the joint distribution of the parameters and data as: 

\begin{eqnarray}
\mu_{i} & = & r-\frac{rx_{i}}{K}\textrm{,}\\
\left[\,r,K,\tau\mid\mathbf{y}\right] & \propto & \prod_{i=1}^{n}\left[\,y_{i}\mid\mu_{i},\tau\right]\left[r\right]\left[K\right]\left[\tau\right]\textrm{,}\nonumber \\
\left[r,K,\tau\mid\mathbf{y}\right] & \propto & \prod_{i=1}^{n}\textrm{normal}\left(y_{i}\mid\mu_{i},\tau\right)\times\label{eq:conditional} \textrm{gamma}\left(K\mid.001,.001\right)\\
 &  &\textrm{gamma}\left(\tau\mid.001,.001\right)\textrm{gamma}\left(r\mid.001,.001\right),\nonumber 
\end{eqnarray}

where the priors are vague distributions for quantities that must, by definition, be positive. Note that we have used the precision $(\tau)$ as a argument to the normal distribution rather than the variance $\left(\tau=\frac{1}{\sigma^{2}}\right)$ to keep things consistent with the code below, a requirement of the BUGS language. Now, we have full, abiding confidence that with a couple of hours worth of work, perhaps less, you could knock out a Gibbs sampler to estimate $r,K,$ and $\tau$. However, we are all for doing things nimbly in 15 minutes that might otherwise take a sweaty hour of hard labor.

```{r, eval=FALSE}
## Logistic example for Primer
model{
  # priors
  K ~ dgamma(.001, .001) # dgamma(r,n)
  r ~ dgamma(.001, .001) # dgamma(r,n)
  tau ~ dgamma(.001, .001) # precision  # dgamma(r,n)
  sigma <- 1/sqrt(tau) # calculate sd from precision
  # likelihood
  for (i in 1:n){
    mu[i] <- r - r/K * x[i]
    y[i] ~ dnorm(mu[i], tau) # dnorm(mu,tau)
  }
}
```

# Exercise 1: Factoring

There is no $x$ in the posterior distribution in equation 4. What are assuming if $x$ is absent? Draw the Bayesian network, or DAG, for this model. Use the chain rule to fully factor the joint distribution into sensible parts then simplify by assuming that $r,K,$ and $\tau$ are independent.

Factoring the joint using the chain rule:

$$
[r,K,\tau,y] = [ y \mid r, K, \tau ] \cdot [r \mid K, \tau]\cdot[K \mid \tau] \cdot[\tau]
$$ 
Assuming $r, K, \tau$ are independent:

$$
[r,K,\tau,y] = [ y \mid r, K, \tau ] \cdot [r]\cdot[K] \cdot[\tau]
$$ 

\textcolor{violet}{There is no $x$ because we are assuming it is measured without error.}

# Exercise 2: Can you improve these priors? 

A recurring theme in this course will be to use priors that are informative whenever possible. The gamma priors in equation 4 include *the entire number line > 0*. Don’t we know more about population biology than that? Let’s, say for now that we are modeling the population dynamics of a large mammal. How might you go about making the priors on population parameters more informative?

\textcolor{violet}{The intrinsic rate of increase $r$ can plausibly take on values between 0 and 1. The only requirement for a vague prior is that its "range of uncertainty should be clearly wider that the range of reasonable values of the parameter". Similarly, we could use experience and knowledge to put some reasonable bounds on $K$ and even $\sigma$, which we can use to calculate $\tau$ as $\tau=\frac{1}{\sigma^{2}}$.}

```{r, eval=FALSE}
## Logistic example for Primer
  model{
  # priors
  K ~ dunif(0, 4000) # dunif(alpha = lower limit, beta = upper limit)
  r ~ dunif (0, 2) # dunif(alpha, beta)
  sigma ~ dunif(0, 2) # dunif(alpha, beta)
  tau <- 1/sigma^2
  # likelihood
  for(i in 1:n){
    mu[i] <- r - r/K * x[i]
    y[i] ~ dnorm(mu[i], tau) # dnorm(mu,tau)
  }
}

```

# Exercise 3: Using `for` loops. 

Write a code fragment to set vague normal priors for 5 regression coefficients -- `dnorm(0, 0.000001)` -- stored in the vector `b`.

```{r, eval=FALSE}
b <- numeric(5)
for(i in 1:length(b)){
  b[i] ~ dnorm(0, 0.000001) # dnorm(mu,tau)
}
```


# Stepping through a JAGS run

We will go through the R code step by step. We start by loading the package `SESYNCBayes` which has the data frame Logistic, which we then order by `PopulationSize`. Next, we specify the initial conditions for the MCMC chain in the statement `inits`. This is exactly the same thing as you did when you wrote MCMC code and assigned a guess to the first element in the chain. Initial conditions must be specified as as “list of lists”, as you can see in the code. If you create a single list, rather than a list of lists, you will get an error message when you execute the `jags.model` statement and your code will not run. Second, this statement allows you to set up multiple chains, which are needed for some tests of convergence and to calculate DIC (more about these tasks later). For example, if you want three chains, you would use:

```{r, eval=FALSE}
inits = list(
  list(K = 1500, r = .2, sigma = 1),
  list(K = 1000, r = .15, sigma = .1),
  list(K = 900, r = .3, sigma = .01)
)
```

Now it is really easy to see why we need the “list of lists” format – there is one list for each chain; but remember, you require the same structure for a single chain, that is, a list of lists. 

Which variables in your JAGS code require initialization? All unknown quantities that appear on the left hand side of the conditioning in the posterior distribution require initial values. Think about it this way. When you were writing your own MCMC algorithm, every chain required a value as the first element in the vector holding the chain. That is what you are doing when you specify initial conditions here. You can get away without explicitly specifying initial values – JAGS will choose them for you if you don’t specify them – however, we strongly urge you to provide explicit initial values, particularly when your priors are vague. This habit also forces you to think about what you are estimating.

The left hand side of the `=` corresponds to variable name for the data in the `JAGS` program and the right hand side of the `=` is what they are called in `R`.

```{r, eval=FALSE}
rm(list = ls())
library(SESYNCBayes)
library(rjags)
# SESYNCBayes which has the data frame Logistic, which we then order by PopulationSize
Logistic = SESYNCBayes::Logistic[order(Logistic$PopulationSize),]
# specify the initial conditions for the MCMC chain 
inits = list(
  list(K = 1500, r = .2, sigma = 1),
  list(K = 1000, r = .15, sigma = .1),
  list(K = 900, r = .3, sigma = .01)
)
# specify the data that will be used by your JAGS program
  #the execution of JAGS is about 5 times faster on double precision than on integers.
hey_data = list(
  n = nrow(SESYNCBayes::Logistic), # n is required in the JAGS program to index the for structure
  x = as.double(SESYNCBayes::Logistic$PopulationSize),
  y = as.double(SESYNCBayes::Logistic$GrowthRate)
)
# specify 3 scalars, n.adapt, n.update, and n.iter
# n.adapt = number of iterations that JAGS will use to choose the sampler 
  # and to assure optimum mixing of the MCMC chain
n.adapt = 1000
# n.update = number of iterations that will be discarded to allow the chain to 
#   converge before iterations are stored (aka, burn-in)
n.update = 10000
# n.iter = number of iterations that will be stored in the 
  # final chain as samples from the posterior distribution
n.iter = 10000
######################
# Call to JAGS
######################
set.seed(1)
jm = rjags::jags.model(
  file = "LogisticJAGS.R"
  , data = hey_data
  , inits = inits
  , n.chains = length(inits)
  , n.adapt = n.adapt
)
stats::update(jm, n.iter = n.update)
zm = rjags::coda.samples(
  model = jm
  , variable.names = c("K", "r", "sigma", "tau")
  , n.iter = n.iter
  , n.thin = 1
)
```

# Exercise 4: Coding the model. 

Write `R` code (algorithm 3) to run the JAGS model (algorithm 2) and estimate the parameters, $r,K,\sigma$ and $\tau$. We suggest you insert the JAGS model into this `R` script using the sink command as shown in algorithm 4 because this model is small. You will find this a convenient way to keep all your code in the same R script. For larger models, you will be happier using a separate file for the JAGS code.

```{r, eval=TRUE}
##################################################################
# insert JAGS model code into an R script
##################################################################
{ # Extra bracket needed only for R markdown files - see answers
  sink("LogisticJAGS.R") # This is the file name for the jags code
  cat("
  ## Logistic example for Primer
    model{
      # priors
      K ~ dunif(0, 4000) # dunif(alpha = lower limit, beta = upper limit)
      r ~ dunif (0, 2) # dunif(alpha, beta)
      sigma ~ dunif(0, 2) # dunif(alpha, beta)
      tau <- 1/sigma^2
      # likelihood
      for(i in 1:n){
        mu[i] <- r - r/K * x[i]
        y[i] ~ dnorm(mu[i], tau) # dnorm(mu,tau)
      }
    }
  ", fill = TRUE)
  sink()
}
##################################################################
# implement model
##################################################################
# SESYNCBayes which has the data frame Logistic, which we then order by PopulationSize
Logistic = SESYNCBayes::Logistic[order(Logistic$PopulationSize),]
# specify the initial conditions for the MCMC chain 
inits = list(
  list(K = 1500, r = .2, sigma = 1),
  list(K = 1000, r = .15, sigma = .1),
  list(K = 900, r = .3, sigma = .01)
)
# specify the data that will be used by your JAGS program
  #the execution of JAGS is about 5 times faster on double precision than on integers.
hey_data = list(
  n = nrow(SESYNCBayes::Logistic), # n is required in the JAGS program to index the for structure
  x = as.double(SESYNCBayes::Logistic$PopulationSize),
  y = as.double(SESYNCBayes::Logistic$GrowthRate)
)
# specify 3 scalars, n.adapt, n.update, and n.iter
# n.adapt = number of iterations that JAGS will use to choose the sampler 
  # and to assure optimum mixing of the MCMC chain
n.adapt = 1000
# n.update = number of iterations that will be discarded to allow the chain to 
#   converge before iterations are stored (aka, burn-in)
n.update = 10000
# n.iter = number of iterations that will be stored in the 
  # final chain as samples from the posterior distribution
n.iter = 10000
######################
# Call to JAGS
######################
set.seed(1)
jm = rjags::jags.model(
  file = "LogisticJAGS.R"
  , data = hey_data
  , inits = inits
  , n.chains = length(inits)
  , n.adapt = n.adapt
)
stats::update(jm, n.iter = n.update)
# save the coda object (more precisely, an mcmc.list object) to R as "zm"
zm = rjags::coda.samples(
  model = jm
  , variable.names = c("K", "r", "sigma", "tau")
  , n.iter = n.iter
  , n.thin = 1
)
#####################
# check output
#####################
MCMCvis::MCMCsummary(zm, params = c("K", "r", "sigma", "tau"))
# chain 1 first 6 iterations and specific columns
zm[[1]][1:6, ]
# zm[[1]][1:6, c("K", "r", "sigma", "tau")] # for specific params
```

# Exercise 5: Understanding coda objects.

1) Convert the coda object `zm`, into a data frame using `df = as.data.frame(rbind(zm[[1]], zm[[2]], zm[[3]]))` Note the double brackets, which effectively unlist each element of `zm`, allowing them to be combined. Another way to do this is `do.call(rbind,zm)`.
2) Look at the first six rows of the data frame.
3) Find the maximum value of $\sigma$.
4) Find the mean of $r$ for the first 1000 iterations.
5) Find the mean of $r$ after the first 1000 iterations.
6) Make two publication quality plots of the marginal posterior density of `K`, one as a smooth curve and the other as a histogram.
7) Compute the probability that $K > 1600$. Hint: what type of probability distribution would you use for this computation? Investigate the the dramatically useful R function `ecdf()`.
8) Compute the probability that $1000 < K < 1600$.
9) Compute the .025 and .975 quantiles of $K$. Hint–use the R `quantile()` function. This is an equal-tailed Bayesian credible interval on K.


## 1) Convert the coda object `zm`, into a data frame using `df = as.data.frame(rbind(zm[[1]], zm[[2]], zm[[3]]))` Note the double brackets, which effectively unlist each element of `zm`, allowing them to be combined. Another way to do this is `do.call(rbind,zm)`.

```{r}
#########################################
#########################################
# only need to do this if want a column denoting the chain #
#########################################
#########################################
# function to transform to data frame and 
  # store chain number and iteration as variable
df_fn <- function(x){
  as.data.frame(zm[[x]]) %>% # if want all columns!
  # as.data.frame(zm[[x]][,c("K", "r", "sigma", "tau")]) %>% # if want specific columns! 
    dplyr::mutate(
      chain = x
      , iteration = dplyr::row_number()
    )
}
# pass zm to function
zm_df <- 1:length(zm) %>% 
  purrr::map(df_fn) %>% # purrr::map returns a list of data frames in this case...
  dplyr::bind_rows() # ...which we bind together
```


## 2) Look at the first six rows of the data frame.

```{r}
zm_df %>% 
  dplyr::slice_head(n = 6)
```


## 3) Find the maximum value of $\sigma$.

```{r}
max(zm_df$sigma)
```


## 4) Find the mean of $r$ for the first 1000 iterations.

```{r}
mean(zm_df$r[zm_df$iteration <= 1000])
```


## 5) Find the mean of $r$ after the first 1000 iterations.

```{r}
mean(zm_df$r[zm_df$iteration > 1000])
```


## 6) Make two publication quality plots of the marginal posterior density of `K`, one as a smooth curve and the other as a histogram.

```{r}
ggplot(
    data = zm_df
    , mapping = aes(x = K)
  ) +
  geom_histogram(
    aes(y = ..density..)
    , bins = 100
    , fill = "navy"
    , alpha = 0.8
    , color = "gray25"
  ) +
  xlab(latex2exp::TeX("$K$ sample value")) +
  ylab("Density") +
  labs(
    title = latex2exp::TeX("Marginal posterior density of $K$")
  ) +
  theme_bw()
```

```{r}
ggplot(
    data = zm_df
    , mapping = aes(x = K)
  ) +
  geom_density(
    aes(y = ..density..)
    , linetype = 2
    , lwd = 1.2
    , color = "navy"
  ) +
  xlab(latex2exp::TeX("$K$ sample value")) +
  ylab("Density") +
  labs(
    title = latex2exp::TeX("Marginal posterior density of $K$")
  ) +
  theme_bw()
```


## 7) Compute the probability that $K > 1600$. Hint: what type of probability distribution would you use for this computation? Investigate the the dramatically useful R function `ecdf()`.

```{r}
# Find the probability that the parameter K exceeds 1600
1 - stats::ecdf(zm_df$K)(1600)
```


## 8) Compute the probability that $1000 < K < 1600$.
```{r}
# Find the probability that the parameter 1000 < K < 1300 
stats::ecdf(zm_df$K)(1300) - stats::ecdf(zm_df$K)(1000)
```


## 9) Compute the .025 and .975 quantiles of $K$. Hint–use the R `quantile()` function. This is an equal-tailed Bayesian credible interval on K.

```{r}
quantile(zm_df$K, probs = c(.025, .975))
```

# Exercise 6: Using `MCMCsummary`
1) Summarize the coda output from the logistic model with 4 significant digits. Include `Rhat` and effective sample size diagnostics (more about these soon).
2) Summarize the coda output for $r$ alone.

## 1) Summarize the coda output from the logistic model with 4 significant digits. Include `Rhat` and effective sample size diagnostics (more about these soon).

```{r}
MCMCvis::MCMCsummary(zm, params = c("K", "r", "sigma", "tau"), digits = 4, Rhat = TRUE, n.eff = TRUE)
```


## 2) Summarize the coda output for $r$ alone.

```{r}
MCMCvis::MCMCsummary(zm, params = c("r"), digits = 4, Rhat = TRUE, n.eff = TRUE)
```

# Exercise 7: Using MCMCchains 

1) Extract the chains for $r$ and $\sigma$ as a data frame using `MCMCchains` and compute their .025 and .975 quantiles from the extracted object. Display three significant digits. 
2) Make a publication quality histogram for the chain for $\sigma$. Indicate the .025 and .975 Bayesian equal-tailed credible interval value with vertical red lines.
3) Overlay the .95 highest posterior density interval with vertical lines in blue. This is a "learn on your own" problem intended to allow you to rehearse the most important goal of this class: being sufficiently confident to figure things out. Load the package `HDInterval`, enter `?HDInterval` at the console and follow your nose. Also see Hobbs and Hooten Figure 8.2.1.


## 1) Extract the chains for $r$ and $\sigma$ as a data frame using `MCMCchains` and compute their .025 and .975 quantiles from the extracted object. Display three significant digits. 

```{r}
temp_df <- MCMCvis::MCMCchains(zm, params = c("r", "sigma")) %>% 
  as.data.frame()
# quantiles
qtiles <- c(.025, .975)
# quantiles of r
qtile_r <- quantile(temp_df$r, probs = qtiles) %>%
  data.frame(
    quantile = scales::percent(qtiles, accuracy = 0.1)
    , r = .
  )
rownames(qtile_r) <- NULL
kableExtra::kable(qtile_r, digits=3) %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position")

# quantiles of sigma
# quantiles of r
qtile_sigma <- quantile(temp_df$sigma, probs = qtiles) %>%
  data.frame(
    quantile = scales::percent(qtiles, accuracy = 0.1)
    , sigma = .
  )
rownames(qtile_sigma) <- NULL
kableExtra::kable(qtile_sigma, digits=3) %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position")

```


## 2) Make a publication quality histogram for the chain for $\sigma$. Indicate the .025 and .975 Bayesian equal-tailed credible interval value with vertical red lines.

```{r}
ggplot(
    data = temp_df
    , mapping = aes(x = sigma)
  ) +
  geom_histogram(
    aes(y = ..density..)
    , bins = 100
    , fill = "steelblue"
    , alpha = 0.8
    , color = "gray25"
  ) +
  geom_vline(
    xintercept = qtile_sigma$sigma
    , color = "firebrick"
    , linetype = "dashed"
    , lwd = 1.1
  ) +
  geom_text(
    data = qtile_sigma
    , mapping = aes(x = sigma*1.025, y = 150, label = quantile)
  ) +
  xlab(latex2exp::TeX("$\\sigma$ sample value")) +
  ylab("Density") +
  labs(
    title = latex2exp::TeX("Marginal posterior density of $\\sigma$")
  ) +
  theme_bw()
```


## 3) Overlay the .95 highest posterior density interval with vertical lines in blue. This is a "learn on your own" problem intended to allow you to rehearse the most important goal of this class: being sufficiently confident to figure things out. Load the package `HDInterval`, enter `?HDInterval` at the console and follow your nose. Also see Hobbs and Hooten Figure 8.2.1.

```{r}
hdi_95 <- HDInterval::hdi(temp_df$sigma, credMass = 0.95)

ggplot(
    data = temp_df
    , mapping = aes(x = sigma)
  ) +
  geom_histogram(
    aes(y = ..density..)
    , bins = 100
    , fill = "steelblue"
    , alpha = 0.8
    , color = "gray25"
  ) +
  geom_vline(
    xintercept = qtile_sigma$sigma
    , color = "firebrick"
    , linetype = "dashed"
    , lwd = 1.1
  ) +
  geom_text(
    data = qtile_sigma
    , mapping = aes(x = sigma*1.025, y = 150, label = quantile)
  ) +
  geom_vline(
    xintercept = hdi_95
    , color = "royalblue"
    , linetype = "dotted"
    , lwd = 1
  ) +
  geom_segment(
    mapping = aes(y = 20, yend = 20, x = hdi_95[1], xend = hdi_95[2])
    , color = "royalblue"
    , lwd = 1.5
  ) +
  xlab(latex2exp::TeX("$\\sigma$ sample value")) +
  ylab("Density") +
  labs(
    title = latex2exp::TeX("Marginal posterior density of $\\sigma$")
  ) +
  theme_bw()

```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(temp_df)
gc()
```


# Exercise 8: Plotting model predictions using the MCMCpstr formatting.

1) Plot the observations of per-capita growth rate as a function of observed population size. Be sure that the population size data are sorted from lowest to highest as described above
2) Overlay the median of the model predictions as a solid black line.
3) Overlay the 95% credible intervals as dashed lines in red.
4) Overlay the 95% highest posterior density intervals as dashed lines in blue.
5) What do you note about these two intervals? Will this always be the case? Why or why not?
6) What do the dashed lines represent? What inferential statement can we make about mu relative these lines?

Presume you would like to get posterior distributions on the *predictions* of your regression model for each of the $x$ values. Note in the JAGS code that we make this prediction as `mu[i] <- r - r / K * x[i]`. Making predictions requires no more than adding `mu` to the list of
random variables being monitored by changing your coda.samples statement to read:

```{r}
zm = coda.samples(jm, variable.names = c("K", "r", "sigma", "mu"),  n.iter = 10000)
```


## 1) Plot the observations of per-capita growth rate as a function of observed population size. Be sure that the population size data are sorted from lowest to highest as described above

```{r}
ggplot(data = hey_data %>% dplyr::bind_cols(), mapping  = aes(x = x, y = y)) + 
  geom_point(color = "gray60") +
  geom_smooth(method = "lm", color = "gray35", alpha = 0.2) +
  xlab("Population Size") +
  ylab("Growth Rate") +
  labs(
    title = "Observed per-capita growth rate as a function of observed population size"
    , subtitle = "*Simple linear regression model estimate shown"
  ) +
  theme_bw()
```


## 2) Overlay the median of the model predictions as a solid black line.

```{r}
# create data frame
temp_df <- dplyr::bind_cols(
    hey_data
    , mu_median = MCMCvis::MCMCpstr(zm, params = "mu", func = median) %>% unlist()
  )
# plot
ggplot(data = temp_df) + 
  geom_point(mapping  = aes(x = x, y = y), color = "gray60") +
  geom_line(mapping  = aes(x = x, y = mu_median), color = "black", lwd = 1.1) +
  xlab("Population Size") +
  ylab("Growth Rate") +
  labs(
    title = "Observed per-capita growth rate as a function of observed population size"
    , subtitle = "*Median of the model predictions shown as line"
  ) +
  theme_bw()

```


## 3) Overlay the 95% credible intervals as dashed lines in red.

```{r}
temp_df <- temp_df %>% 
  dplyr::bind_cols(
    MCMCvis::MCMCpstr(zm, params = "mu", func = function(x) quantile(x, c(0.025, 0.975))) %>% as.data.frame()
  )

# plot
ggplot(data = temp_df) + 
  geom_point(mapping  = aes(x = x, y = y), color = "gray60") +
  geom_line(mapping  = aes(x = x, y = mu_median), color = "black", lwd = 1.1) +
  geom_line(mapping  = aes(x = x, y = mu.2.5.), color = "firebrick", lwd = 1, linetype = "dashed") +
  geom_line(mapping  = aes(x = x, y = mu.97.5.), color = "firebrick", lwd = 1, linetype = "dashed") +
  xlab("Population Size") +
  ylab("Growth Rate") +
  labs(
    title = "Observed per-capita growth rate as a function of observed population size"
    , subtitle = "*Median of the model predictions and 95% credible intervals shown"
  ) +
  theme_bw()

```


## 4) Overlay the 95% highest posterior density intervals as dashed lines in blue.

```{r}
temp_df <- temp_df %>% 
  dplyr::bind_cols(
    MCMCvis::MCMCpstr(zm, params = "mu", func = function(x) HDInterval::hdi(x, credMass = 0.95)) %>% as.data.frame()
  )

# plot
ggplot(data = temp_df) + 
  geom_point(mapping  = aes(x = x, y = y), color = "gray60") +
  geom_line(mapping  = aes(x = x, y = mu_median), color = "black", lwd = 1.1) +
  geom_line(mapping  = aes(x = x, y = mu.2.5.), color = "firebrick", lwd = 1.1, linetype = "dashed") +
  geom_line(mapping  = aes(x = x, y = mu.97.5.), color = "firebrick", lwd = 1.1, linetype = "dashed") +
  geom_line(mapping  = aes(x = x, y = mu.upper), color = "royalblue", lwd = 1, linetype = "dashed") +
  geom_line(mapping  = aes(x = x, y = mu.lower), color = "royalblue", lwd = 1, linetype = "dashed") +
  xlab("Population Size") +
  ylab("Growth Rate") +
  labs(
    title = "Observed per-capita growth rate as a function of observed population size"
    , subtitle = "*Median of the model predictions, 95% credible intervals, & 95% highest posterior density intervals shown"
  ) +
  theme_bw() +
  theme(
    plot.subtitle = element_text(size = 9)
  )

```


## 5) What do you note about these two intervals? Will this always be the case? Why or why not?

\textcolor{violet}{The intervals are exactly overlapping in this particular case. Such overlap will not always occur. Equal-tailed intervals based on quantiles will be broader than highest posterior density intervals when marginal posterior distributions are asymmetric.}

## 6) What do the dashed lines represent? What inferential statement can we make about mu relative these lines?

\textcolor{violet}{There is a 95% probability that the true value of the mean per-capita population growth rate falls between these lines. Not that this differs from the prediction of a new observation of population growth rate, which would have much broader credible intervals.}

# Exercise 9: Creating caterpillar plots with MCMCplot. 

Use the `MCMCplot` function to create caterpillar plots to visualize the posterior densities for mu using the coda object zm from earlier. Then use `?MCMCplot` to explore different plotting options. There are lots of these options, including ways to make the plots publication quality, show overlap with zero, etc.

```{r}
MCMCvis::MCMCplot(zm, params = "mu")
```

# Exercise 10: Assessing convergence. 

Rerun the logistic model with n.adapt = 100.

```{r}
n.adapt = 100
jm.short = jags.model("LogisticJAGS.R", data = hey_data, inits = inits, n.chains = length(inits), n.adapt = n.adapt)
```

1) Keep the next 500 iterations. Assess convergence visually with MCMCtrace and with the Gelman-Rubin, Heidelberger and Welch, and Raftery diagnostics. 

```{r}
n.iter = 500
zm.short = coda.samples(jm.short, variable.names = c("K", "r", "sigma", "tau"), n.iter = n.iter, n.thin = 1)
MCMCvis::MCMCtrace(zm.short, pdf = FALSE)
# Gelman-Rubin diagnostic
coda::gelman.diag(zm.short)
# Heidelberger and Welch diagnostic
coda::heidel.diag(zm.short)
# Raftery diagnostic
coda::raftery.diag(zm.short)
```

2. Update another 500 iterations and then keep 500 more iterations. Repeat your assessment of convergence. 

```{r}
n.update = 500
update(jm.short, n.iter = n.update)
n.iter = 500
zm.short = coda.samples(jm.short, variable.names = c("K", "r", "sigma", "tau"), n.iter = n.iter, n.thin = 1)
MCMCvis::MCMCtrace(zm.short, pdf = FALSE)
# Gelman-Rubin diagnostic
coda::gelman.diag(zm.short)
# Heidelberger and Welch diagnostic
coda::heidel.diag(zm.short)
# Raftery diagnostic
coda::raftery.diag(zm.short)
```


3. Repeat steps 1 and 2 until you feel you have reached convergence. 

```{r}
n.update = 10000
update(jm.short, n.iter = n.update)
n.iter = 5000
zm.short = coda.samples(jm.short, variable.names = c("K", "r", "sigma", "tau"), n.iter = n.iter, n.thin = 1)
MCMCvis::MCMCtrace(zm.short, pdf = FALSE)
# Gelman-Rubin diagnostic
coda::gelman.diag(zm.short)
# Heidelberger and Welch diagnostic
coda::heidel.diag(zm.short)
# Raftery diagnostic
coda::raftery.diag(zm.short)
```



4. Change the adapt phase to zero and repeat steps 1 – 4. What happens?

